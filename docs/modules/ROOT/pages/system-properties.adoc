= System Properties
[[appendix]]

The table below lists the system properties with their descriptions in alphabetical order.
See xref:configuration:configuring-with-system-properties.adoc[Configuring with System Properties] to learn
how to configure Hazelcast using these properties.

NOTE: When you want to reconfigure a system property,
you need to restart the members for which the property is modified.

[cols="4,1,1,4a"]
.System Properties
|===
|Property Name
| Default Value
| Type
| Description

|`hazelcast.aggregation.accumulation.parallel.evaluation`
|true
|bool
|Specifies whether to run the aggregation accumulation for multiple entries in parallel.
Each Hazelcast member executes the accumulation stage of an
aggregation using a single thread by default. In most cases it is useful to do it in parallel.

|`hazelcast.backpressure.backoff.timeout.millis`
|60000
|int
|Controls the maximum timeout in milliseconds to wait for an invocation space to be available.
The value needs to be equal to or larger than 0.

|`hazelcast.backpressure.enabled`
|false
|bool
|Enable back pressure.

|`hazelcast.backpressure.max.concurrent.invocations.per.partition`
|100
|int
|The maximum number of concurrent invocations per partition.

|`hazelcast.backpressure.syncwindow`
|1000
|string
|Used when back pressure is enabled.
The larger the sync window value, the less frequent an asynchronous backup is converted to a sync backup.

|`hazelcast.cache.invalidation.batch.enabled`
|true
|bool
|Specifies whether the cache invalidation event batch sending is enabled or not.

|`hazelcast.cache.invalidation.batch.size`
|100
|int
|Defines the maximum number of cache invalidation events to be drained and sent to the event listeners in a batch.

|`hazelcast.cache.invalidation.batchfrequency.seconds`
|5
|int
|Defines cache invalidation event batch sending frequency in seconds.

| `hazelcast.client.cleanup.period.millis`
| 10000
| int
| Period, in milliseconds, to check if a client is still part of the cluster.

| `hazelcast.client.cleanup.timeout.millis`
| 120000
| int
| Timeout duration to decide if a client is still part of the cluster.
If a member cannot find any connection to a client in the cluster,
it cleans up the local resources that are owned by that client.

|[[client-max-no]] `hazelcast.client.max.no.heartbeat.seconds`
|300
|int
|Time after which the member assumes the client is dead and closes its connections to the client.

|[[client-tpc-enabled]] `hazelcast.client.tpc.enabled`
|false
|bool
|Enables TPC (Thread-Per-Core) on client.

|[[client-tpc-eventloop]] `hazelcast.client.tpc.eventloop`
|12
|int
|Specifies maximum number of eventloops.

|`hazelcast.client.protocol.max.message.bytes`
| 1024
|int
| Client protocol message size limit (in bytes) for unverified connections. I.e. maximal length of the client authentication message.

|`hazelcast.clientengine.blocking.thread.count`
|-1
|int
| Number of threads that the client engine has available for
processing requests that are blocking, e.g., transactions. When
not set, it is set as the value of core size * 20.

|`hazelcast.clientengine.query.thread.count`
|
|int
| Number of threads to process query requests coming from the clients.
Default count is the number of cores multiplied by 1.

|`hazelcast.clientengine.thread.count`
|
|int
|Maximum number of threads to process non-partition-aware client requests, like `map.size()`, executor tasks, etc.
Default count is the number of cores multiplied by 20.

|`hazelcast.cluster.shutdown.timeout.seconds`
|900
|int
|Once a member initiates a cluster shutdown, it waits for all members to leave the cluster before shutting itself down.
This parameter defines a timeout after which there may still be some members in the cluster but the initiator will give up and shut itself down regardless.

|`hazelcast.cluster.version.auto.upgrade.enabled`
| false
| bool
| Specifies whether the automatic cluster version upgrading is enabled.

|`hazelcast.cluster.version.auto.upgrade.min.cluster.size`
| 1
| int
| When set to a value greater than 1, automatic upgrading waits to reach that cluster size to proceed.

|`hazelcast.concurrent.window.ms`
|100
|int
|Property needed for concurrency detection so that write through can be done correctly.
This property sets the time window, in milliseconds, between the concurrency detection
and its notification. Normally in a concurrent system, the window
keeps sliding forward so it always remains concurrent.
Setting it too high effectively disables the optimization because once
a concurrency is detected it will keep that way. Setting it too low
could lead to suboptimal performance because the system
will try write through and other optimizations even though the system is concurrent.

|`hazelcast.config.instance.tracking.file`
|
|string
|The JVM system property which gets a value as the full path of the instance tracking file for each Hazelcast member.
It can then be used to verify the name and location of this file. See xref:maintain-cluster:monitoring.adoc#instance-tracking[Instance Tracking]

|`hazelcast.connect.all.wait.seconds`
| 120
| int
| Timeout to connect all other cluster members when a member is joining to a cluster.

|`hazelcast.connection.monitor.interval`
| 100
| int
| Minimum interval in milliseconds to consider a connection error as critical.

|`hazelcast.connection.monitor.max.faults`
| 3
| int
| Maximum I/O error count before disconnecting from a member.

|`hazelcast.data.search.dynamic.config.first.enabled`
|false
|bool
|Enables performing search for the data structures within the dynamic configurations first. Default behavior is searching within the static configurations.

|`hazelcast.diagnostics.directory`
|`user.dir`
|string
|Output directory of the diagnostic log files.

NOTE: For detailed information about the diagnostic tool,
along with this and the following diagnostic related system properties, see the xref:maintain-cluster:monitoring.adoc#diagnostics[Diagnostics section].

|`hazelcast.diagnostics.enabled`
|false
|bool
|Specifies whether diagnostics tool is enabled or not for the cluster.

|`hazelcast.diagnostics.filename.prefix`
|
|string
|Optional prefix for the diagnostics log file.

|`hazelcast.diagnostics.invocation.sample.period.seconds`
|0
|long
|Frequency of scanning all the pending invocations in seconds.
0 means the `Invocations` plugin for diagnostics tool is disabled.

|`hazelcast.diagnostics.invocation.slow.threshold.seconds`
|5
|long
|Threshold period, in seconds, that makes an invocation to be considered as slow.

|`hazelcast.diagnostics.max.rolled.file.count`
|10
|int
|Allowed count of diagnostic files within each roll.

|`hazelcast.diagnostics.max.rolled.file.size.mb`
|50
|int
| Size of each diagnostic file to be rolled.

|`hazelcast.diagnostics.member-heartbeat.max-deviation-percentage`
|100
|int
|Maximum allowed deviation for a member-to-member heartbeats.

|`hazelcast.diagnostics.member-heartbeat.period.seconds`
|10
|long
|Period for which the MemberHeartbeats plugin of the diagnostics tool runs.
0 means this plugin is disabled.


|`hazelcast.diagnostics.memberinfo.period.second`
|60
|long
|Frequency, in seconds, at which the cluster information is dumped to the diagnostics log file.

|`hazelcast.diagnostics.metrics.period.seconds`
|60
|long
|Frequency, in seconds, at which the Metrics plugin dumps information to the diagnostics log file.

|`hazelcast.diagnostics.operation-heartbeat.max-deviation-percentage`
|33
|int
|Maximum allowed deviation for a member-to-member operation heartbeats.

|`hazelcast.diagnostics.operation-heartbeat.seconds`
|10
|long
|Period, in seconds, for which the OperationHeartbeats plugin of the diagnostics tool runs.
0 means this plugin is disabled.

|`hazelcast.diagnostics.pending.invocations.period.seconds`
|0
|long
|Period, in seconds, for which the PendingInvocations plugin of the diagnostics tool runs.
0 means this plugin is disabled.

|`hazelcast.diagnostics.stdout`
|`FILE`
|string
|Configures the output for the diagnostics. Available values are `FILE`, `STDOUT`, and `LOGGER`.

|`hazelcast.diagnostics.slowoperations.period.seconds`
|60
|long
| Period, in seconds, for which the SlowOperations plugin of the diagnostics tool runs.
0 means this plugin is disabled.

|`hazelcast.diagnostics.storeLatency.period.seconds`
|0
|long
|Period, in seconds, for which the StoreLatency plugin of the diagnostics tool runs.
0 means this plugin is disabled.

|`hazelcast.diagnostics.storeLatency.reset.period.seconds`
|0
|long
|Period, in seconds, for resetting the statistics for the StoreLatency plugin of the diagnostics tool.

|`hazelcast.diagnostics.systemlog.enabled`
|true
|bool
|Specifies whether the SystemLog plugin of the diagnostics tool is enabled or not.

|`hazelcast.diagnostics.systemlog.partitions`
|false
|bool
|Specifies whether the SystemLog plugin collects information about partition migrations.

|`hazelcast.discovery.enabled`
|false
|bool
|Enables/disables the Discovery SPI lookup over the old native implementations.
See xref:extending-hazelcast:discovery-spi.adoc[] for more information.

|`hazelcast.discovery.public.ip.enabled`
| false
| bool
| Enable use of public IP address in member discovery with Discovery SPI.
If you set this property to true in your source cluster, please make sure you have set the public addresses for your
target members since they will be discovered using their public addresses. Otherwise, they cannot be discovered.
See the xref:clusters:network-configuration.adoc#public-address[Public Address section].

|[[ignore-dynamic-conf-conflicts]] `hazelcast.dynamicconfig.ignore.conflicts`
|
|bool
| When it is set to `true`, Hazelcast does not check if the added dynamic configuration
conflicts with an existing dynamic configuration. Existing dynamic configuration
is not replaced. Note that, this property is not meant to ignore the conflicts between
static configuration and dynamic configuration. If the added dynamic configuration exists
in the static configuration of the member, it will throw an exception regardless this property.

|`hazelcast.enterprise.license.key`
| null
| string
| link:https://hazelcast.com/products/[Hazelcast Enterprise^] license key.

|`hazelcast.event.queue.capacity`
| 1000000
| int
| Capacity of internal event queue.

|`hazelcast.event.queue.timeout.millis`
| 250
| int
| Timeout to enqueue events to event queue.

|`hazelcast.event.sync.timeout.millis`
|5000
|int
| To prevent overloading of the outbound connections,
once in a while an event is made synchronous by wrapping it in a
dummy operation and waiting for a dummy response. This causes
the outbound write queue of the connection to get drained.
This timeout configures the maximum amount of waiting time for this response.
Setting it to a too low value can lead to an uncontrolled growth
of the outbound write queue of the connection.

|`hazelcast.event.thread.count`
| 5
| int
| Number of event handler threads.

|[[hazelcast.graceful.shutdown.max.wait]]`hazelcast.graceful.shutdown.max.wait`
| 600
| int
| Maximum wait in seconds during graceful shutdown.

|`hazelcast.hd.global.index.enabled`
|true
|bool
|Specifies whether the global concurrent High-Density Memory Store
indexes are enabled or not.

|`hazelcast.health.monitoring.delay.seconds`
|20
|int
|Health monitoring logging interval in seconds. NOTE: For detailed information about
the health monitoring tool, along with this and the following health monitoring related system properties,
see the xref:maintain-cluster:monitoring.adoc#health-check-monitoring[Health Check and Monitoring section].

|`hazelcast.health.monitoring.level`
|SILENT
|string
|Health monitoring log level. When *SILENT*, logs are printed only when values exceed some predefined threshold.
When *NOISY*, logs are always printed periodically. Set *OFF* to turn off completely.

|`hazelcast.health.monitoring.threshold.cpu.percentage`
|70
|int
|When the health monitoring level is *SILENT*, logs are printed only when the CPU usage exceeds this threshold.

|`hazelcast.health.monitoring.threshold.memory.percentage`
|70
|int
|When the health monitoring level is *SILENT*, logs are printed only when the memory usage exceeds this threshold.

|`hazelcast.heartbeat.failuredetector.type`
|`deadline`
|string
|Type of the heartbeat failure detector. See the
xref:clusters:failure-detector-configuration.adoc[Failure Detector Configuration section].

|`hazelcast.heartbeat.interval.seconds`
| 5
| int
| Heartbeat send interval in seconds.

|`hazelcast.hidensity.check.freememory`
|true
|bool
|If enabled and is able to fetch memory statistics via Java's `OperatingSystemMXBean`,
it checks whether there is enough free physical memory for the requested number of bytes.
If the free memory checker is disabled (false), acts as if the check is succeeded.

|`hazelcast.hotrestart.free.native.memory.percentage` (deprecated)
|15
|long
|Percentage of the free memory space that is required for xref:storage:persistence.adoc[Persistence].

|`hazelcast.ignoreXxeProtectionFailures`
|false
|bool
|If enabled and when a problem occurs during enabling XML External Entity (XXE) protection, then the problem is ignored and only a warning message is logged.

This property should only be used as a last resort.
Hazelcast uses the XXE protection by setting respective XML processor properties. These properties are supported in modern XML processors, e.g., the default one available in Java. An old processor on the classpath, such as Xerces and Xalan, may miss the support and  throw an exception during enabling the XXE protection. Setting this system property to `true` allows ignoring such exceptions.

|`hazelcast.index.copy.behavior`
|COPY_ON_READ
| string
| Defines the behavior for index copying on index read/write.
See the xref:query:how-distributed-query-works.adoc#copying-indexes[Copying Indexes section].

|`hazelcast.init.cluster.version`
|
|string
|Used to override the cluster version to use while a Hazlecast instance is not yet part of a cluster. The cluster version assumed before joining
a cluster may affect the serialization format of the cluster discovery.
The default is to use the member's codebase version. You may
need to override it for your member to join a cluster running on a
previous cluster version.

|`hazelcast.initial.min.cluster.size`
| 0
| int
| Initial expected cluster size to wait before member to start completely.

|`hazelcast.initial.wait.seconds`
| 0
| int
| Initial time in seconds to wait before member to start completely.

|`hazelcast.internal.map.expiration.cleanup.operation.count`
|N/A
|int
|Count of scannable partitions in each run of the background expiration task. No default value exists. It is
dynamically calculated against the partition count or partition thread count.

|`hazelcast.internal.map.expiration.cleanup.percentage`
|10
|int
|Scannable percentage of the entries in the maps' partitions in each run of the background expiration task.

|`hazelcast.internal.map.expiration.task.period.seconds`
|5
|int
|Interval, in seconds, at which the background expiration task is going to run.

|`hazelcast.internal.map.expired.key.scan.timeout.nanos`
|1000000
|int
|Timeout for a partition of a map. Its default value is 1000000 nanoseconds (1 ms).
It puts an upper limit for execution of a background expiry operations to prevent the
high CPU usages when a partition's size grows.

|[[internal-tpc-enabled]] `hazelcast.internal.tpc.enabled`
|false
|bool
|Enables TPC (Thread-Per-Core) on member.

|[[internal-tpc-eventloop]] `hazelcast.internal.tpc.eventloop`
|12
|int
|Specifies maximum number of eventloops.

|`hazelcast.invalidation.max.tolerated.miss.count`
|10
|int
|If missed invalidation count is bigger than this value, relevant cached data is made unreachable.

|`hazelcast.invalidation.reconciliation.interval.seconds`
|60
|int
|Period for which the cluster members are scanned to compare generated invalidation events with the received ones from Near Cache.

|`hazelcast.invocation.max.retry.count`
|250
|int
| Maximum number of retries for an invocation. After threshold is reached,
the invocation is assumed as failed.

|`hazelcast.invocation.retry.pause.millis`
|500
|int
|Pause time between each retry cycle of an invocation in milliseconds.

|`hazelcast.io.balancer.interval.seconds`
|20
|int
|Interval in seconds between IOBalancer executions.

|`hazelcast.io.input.thread.count`
| 3
| int
| Number of socket input threads.

|`hazelcast.io.output.thread.count`
| 3
| int
| Number of socket output threads.

|`hazelcast.io.thread.count`
| 3
| int
| Number of threads performing socket input and socket output.
If, for example, the default value (3) is used, it means there are 3 threads performing input and 3 threads performing output (6 threads in total).

|`hazelcast.io.write.through`
|true
|bool
|Optimization that allows sending of packets over the network to be done on the calling thread if the
conditions are right. This can reduce the latency and increase the performance for low threaded environments.

|`hazelcast.jcache.provider.type`
|
|string
|Type of the JCache provider. Values can be `client` or `server`.

|`hazelcast.jmx`
| false
| bool
| Enable xref:maintain-cluster:monitoring.adoc#monitoring-with-jmx[JMX] agent.

|`hazelcast.jet.custom.lib.dir`
|`custom-lib`
|string
|The directory containing the JAR files, that can be used to specify custom classpath for a stage in a pipeline.

|`hazelcast.jet.idle.cooperative.min.microseconds`
|25
|int
|The minimum time in microseconds the cooperative worker threads will sleep if none of the tasklets made any progress. Lower values increase
idle CPU usage but may result in decreased latency. Higher values will increase latency, and very high values (>10000µs) will also limit throughput.

|`hazelcast.jet.idle.cooperative.max.microseconds`
|500
|int
|The maximum time in microseconds the cooperative worker threads will sleep if none of the tasklets made any progress. Lower values increase
idle CPU usage but may result in decreased latency. Higher values will increase latency, and very high values (>10000µs) will also limit the throughput.

|`hazelcast.jet.idle.noncooperative.min.microseconds`
|25
|int
|The minimum time in microseconds the non-cooperative worker threads will sleep if none of the tasklets made any progress. Lower values increase
idle CPU usage but may result in decreased latency. Higher values will increase latency, and very high values (>10000µs) will also limit the throughput.

|`hazelcast.jet.idle.noncooperative.max.microseconds`
|5000
|int
|The maximum time in microseconds the non-cooperative worker threads will sleep if none of the tasklets made any progress. Lower values increase
idle CPU usage but may result in decreased latency. Higher values will increase latency, and very high values (>10000µs) will also limit the throughput.

|`hazelcast.jet.job.results.max.size`
|1000
|int
|Maximum number of job results to keep in the cluster; the oldest results will be automatically deleted after this size is reached.

|`hazelcast.jet.job.results.ttl.seconds`
|604800
|int
|Maximum number of time in seconds the job results are kept in the cluster. They will be automatically deleted after this period is reached.

|`hazelcast.jet.job.scan.period`
|5000
|int
|The Jet engine periodically checks for new jobs to start and perform cleanup of the unused resources. This property configures how often this check and cleanup is done, in milliseconds.

|`hazelcast.jmx.update.interval.seconds`
|5
|int
|Some JMX MBeans are cached  to reduce the overhead of calculating statistics. This parameter determines for how long the MBeans can go stale.

|`hazelcast.local.localAddress`
|
| string
| It is an overrider property for the default server socket listener's IP address.
If this property is set, then this is the address where the server socket is bound to.

|`hazelcast.local.publicAddress`
|
| string
| It is an overrider property for the default public address to be advertised to other cluster members and clients.

|`hazelcast.lock.max.lease.time.seconds`
|Long.MAX_VALUE
| long
| All locks which are acquired without an explicit lease time use this value (in seconds) as the lease time.
When you want to set an explicit lease time for your locks, you cannot set it to a longer time than this value.

|`hazelcast.logging.details.enabled`
|true
|bool
|Specifies whether the cluster name, IP and version should be included in
all log messages.

|`hazelcast.logging.emoji.enabled`
|false
|bool
|Specifies whether cluster emojis can be used in log messages. This is just a hint for components calling the logging.

|`hazelcast.logging.type`
| jdk
| enum
| Name of xref:clusters:logging-configuration.adoc[logging] framework type to send logging events.

|`hazelcast.map.entry.filtering.natural.event.types`
| false
| bool
| Notify xref:data-structures:listening-for-map-entries.adoc[entry listeners with predicates] on map entry updates with
events that match entry, update or exit from predicate value space.

|`hazelcast.map.eviction.batch.size`
|1
|int
|Maximum number of IMap entries Hazelcast will evict during a
single eviction cycle. Eviction cycle is triggered by a map
mutation. Typically it is fine to evict at most a single entry.
However, when you insert values in a
loop, each iteration doubles the entry size. In this
situation more than just a single entry should be evicted.

|`hazelcast.map.eviction.sample.count`
|15
|int
| Count of the IMap entries in the entry set formed by
random samplings from which Hazelcast chooses to remove the optimal entry
during an IMap eviction.

|`hazelcast.map.expiry.delay.seconds`
|10
|int
|Delays expiration of backup map entries by the defined amount.
This may be useful to prevent some cases where an entry might be observed
on the primary replica (partition owner) but not on the backup replica.
For instance, when running an entry processor on both primary and backup replicas.

|`hazelcast.map.invalidation.batchfrequency.seconds`
| 10
| int
|  If the collected invalidations do not reach the configured batch size, a background process sends them at this interval.

|`hazelcast.map.invalidation.batch.enabled`
| true
| bool
|  Enable or disable batching. When it is set to `false`, all invalidations are sent immediately.

|`hazelcast.map.invalidation.batch.size`
| 100
| int
| Maximum number of invalidations in a batch.

|`hazelcast.map.load.chunk.size`
| 1000
| int
| Maximum size of the key batch sent to the partition owners for value loading and
the maximum size of a key batch for which values are loaded in a single partition.

|`hazelcast.map.loaded.key.limit.per.node`
| 50000
| int
| Maximum number of keys read into memory at a time.
This is a per-member limit to prevent `OutOfMemoryError` when
multiple maps are tried to load concurrently from the same member.

|`hazelcast.map.replica.scheduled.task.delay.seconds`
| 10
| int
| Scheduler delay for map tasks those are executed on backup members.

|[[hazelcast-map-write-behind-queue-capacity]]`hazelcast.map.write.behind.queue.capacity`
|50000
|string
|Maximum write-behind queue capacity per member. It is the total of all write-behind queue sizes in a member including backups.
Its maximum value is `Integer.MAX_VALUE`.
The value of this property is taken into account only if the `write-coalescing` element of the
Map Store configuration is `false`. See xref:mapstore:configuration-guide.adoc[] for the description of the `write-coalescing` element.

|`hazelcast.mastership.claim.timeout.seconds`
|120
|int
|The timeout for which the master member candidate gives up waiting for a response to its mastership claim. After timeout happens, non-responding member will be removed from the member list.

|`hazelcast.max.join.merge.target.seconds`
|20
|int
|Split-brain merge timeout for a specific target.

|`hazelcast.max.join.seconds`
|300
|int
| Join timeout, maximum time to try to join before giving.

|`hazelcast.max.no.heartbeat.seconds`
| 60
| int
| Maximum timeout of heartbeat in seconds for a member to assume it is dead.

CAUTION: Setting this value too low may cause members to be evicted from the cluster when
they are under heavy load: they will be unable to send heartbeat operations in time, so other members will assume that it is dead.

|`hazelcast.max.wait.seconds.before.join`
| 20
| int
| Maximum wait time before join operation.
This is an upper limit on the cluster's pre-join phase duration. The pre-join
phase starts when the master receives the first join request, and ends after
no new members have tried to join for `hazelcast.wait.seconds.before.join`
seconds, or after this upper limit elapsed (whichever comes first). Once the
pre-join phase ends, the master moves into the join phase, during which it
will only admit members that have already tried joining during the pre-join
phase and are still trying to. Once the join phase is complete, the master
will again start admitting new members. See `hazelcast.async.join.strategy.enabled` for how this behaviour can change.

|`hazelcast.mc.executor.thread.count`
|int
|2
|Number of threads that the Management Center service has available
for processing the operations sent from the connected Management Center instance.

|`hazelcast.mc.max.visible.slow.operations.count`
|10
|int
|Management Center maximum visible slow operations count.

|`hazelcast.member.demote.max.wait`
| 600
| int
| Maximum wait duration in seconds during the demotion of a data member to a lite member.

|`hazelcast.member.list.publish.interval.seconds`
| 60
| int
| Interval at which master member publishes a member list.

|`hazelcast.member.naming.moby.enabled`
| true
| bool
| Defines whether the Moby naming should be used for generating instance
names when they are not provided by user. Moby name is a short human-readable
name consisting of a randomly chosen adjective and the surname of a famous person.
If set to `true`, a Moby name is generated. Otherwise, a name that is concatenation
of a static prefix, number and cluster name is provided.

|[[splitbrain-first-check]]`hazelcast.merge.first.run.delay.seconds`
| 300
| int
| Initial run delay of xref:network-partitioning:network-partitioning.adoc#split-brain-syndrome[split-brain/merge process] in seconds.

|[[splitbrain-check-interval]]`hazelcast.merge.next.run.delay.seconds`
| 120
| int
| Run interval of xref:network-partitioning:network-partitioning.adoc#split-brain-syndrome[split-brain/merge process] in seconds.

|`hazelcast.metrics.collection.frequency`
| 5
| int
| Frequency, in seconds, of the xref:maintain-cluster:monitoring.adoc#metrics[metrics] collection cycle. Note that
the preferred way for controlling this setting is xref:maintain-cluster:monitoring.adoc#metrics#metrics-configuration[Metrics Configuration].

|`hazelcast.metrics.datastructures.enabled`
|true
|bool
| Specifies whether collecting metrics from the distributed data structures is enabled.

|`hazelcast.metrics.debug.enabled`
| false
| bool
| Enables collecting debug metrics if set to true, disables it otherwise. Note that this can be set with system property only.

|`hazelcast.metrics.enabled`
| true
| bool
| Enables the xref:maintain-cluster:monitoring.adoc#metrics[metrics collection] if set to `true`, disables it otherwise. Note that the preferred way for
controlling this setting is xref:maintain-cluster:monitoring.adoc#metrics#metrics-configuration[Metrics Configuration].

|`hazelcast.metrics.mc.enabled`
| true
| bool
| Enables buffering the collected xref:maintain-cluster:monitoring.adoc#metrics[metrics] for Management Center if set to `true`, disables it otherwise. Note that
the preferred way for controlling this setting is xref:maintain-cluster:monitoring.adoc#metrics#metrics-configuration[Metrics Configuration].

|`hazelcast.metrics.mc.retention`
| 5
| int
| Duration, in seconds, that the xref:maintain-cluster:monitoring.adoc#metrics[metrics] are retained for Management Center. Note that
the preferred way for controlling this setting is xref:maintain-cluster:monitoring.adoc#metrics#metrics-configuration[Metrics Configuration].

|`hazelcast.metrics.jmx.enabled`
| true
| bool
| Enables exposing the collected xref:maintain-cluster:monitoring.adoc#metrics[metrics] over JMX if set to `true`, disables it otherwise. Note that
the preferred way for controlling this setting is xref:maintain-cluster:monitoring.adoc#metrics#metrics-configuration[Metrics Configuration].

|`hazelcast.multicast.group`
|N/A
|long
|IP address of a multicast group. If not set, then the configuration is read from the xref:clusters:network-configuration.adoc#multicast-element[multicast configuration].

|`hazelcast.multicast.socket.set.interface`
|N/A
|long
|Allows explicit control if the `setInterface()` method is called in the Hazelcast multicast discovery service.
This configuration may affect the multicast behavior on some platforms. The default value is not specified here, and, in such case, Hazelcast multicast service itself
decides if `setInterface()` should be called.

|`hazelcast.network.stats.refresh.interval.seconds`
|3
|int
| Interval, in seconds, at which the network statistics (bytes sent and received)
are re-calculated and published. It is valid only when
xref:clusters:network-configuration.adoc[advanced networking] is used.

|`hazelcast.nio.tcp.spoofing.checks`
| false
| bool
| Controls whether more strict checks upon BIND requests towards a cluster member are applied.
The checks mainly validate the remote BIND request against the remote address as found in the socket.
By default they are disabled, to avoid connectivity issues when deployed under NAT'ed infrastructure.

|`hazelcast.operation.backup.timeout.millis`
|5000
|int
|Maximum time a caller to wait for backup responses of an operation.
After this timeout, operation response is returned to the caller even no backup response is received.

|`hazelcast.operation.call.timeout.millis`
| 60000
| int
| Timeout to wait for a response when a remote call is sent, in milliseconds.

|`hazelcast.operation.fail.on.indeterminate.state`
| false
| bool
| When enabled, an operation fails with `IndeterminateOperationStateException`,
if it does not receive backup acks in time with respect to backup configuration of
its data structure, or the member which owns primary replica of the target partition leaves the cluster.

|`hazelcast.operation.generic.thread.count`
| 2
| int
| Number of generic operation handler threads for each Hazelcast member.
Its default value is the maximum of `2` and `processor count / 2`.

|`hazelcast.operation.priority.generic.thread.count`
| 1
| int
| Number of priority generic operation handler threads per member.
Having at least 1 priority generic operation thread helps to improve
cluster stability since a lot of cluster operations are generic priority
operations and they should get executed as soon as possible. If there is
a dedicated generic operation thread then these operations don't get delayed
because the generic threads are busy executing regular user operations.
So unless memory consumption is an issue, make sure there is at least 1 thread.

|`hazelcast.operation.response.thread.count`
|2
|int
| Number of threads the process responses.
The default value gives stable and good performance.
If set to 0, the response threads are bypassed and the
response handling is done on the IO threads. Under certain
conditions this can give a higher throughput.

|`hazelcast.operation.responsequeue.idlestrategy`
|block
|string
|Specifies whether the response thread for internal operations on the member side are blocked or not.
If you use `block` (the default value) the thread is blocked and need to be notified which can cause
a reduction in the performance. If you use `backoff` there is no blocking.
By enabling the backoff mode and depending on your use case, you can get a 5-10% performance improvement.
However, keep in mind that this increases the CPU utilization.
We recommend you to use backoff with care and if you have a tool for measuring your cluster's performance.

|`hazelcast.operation.thread.count`
| 2
| int
| Number of partition based operation handler threads for each Hazelcast member.
Its default value is the maximum of `2` and count of available processors.

|`hazelcast.partial.member.disconnection.resolution.algorithm.timeout.seconds`
|5
|int
|Timeout, in seconds, to stop the execution of resolution algorithm when needed,
in the case of lots of possible random network disconnections especially
in the large clusters.

|`hazelcast.partial.member.disconnection.resolution.heartbeat.count`
|0
|int
|When the master (oldest member in the cluster) receives a heartbeat
problem report from another member, it first waits for a number
of heartbeat rounds to allow other members
to report their problems, if there is any. This property sets the number
of these rounds.

|`hazelcast.partition.backup.sync.interval`
|30
|int
|Interval for syncing backup replicas in seconds.

|`hazelcast.partition.count`
| 271
| int
| Total partition count.

|`hazelcast.partition.max.parallel.migrations`
|10
|int
|Maximum number of partition migrations to be executed concurrently on a member.
Member can be either source or target of the migration. Having too much parallelization
can increase the heap memory usage and overload the network during a partition rebalance.
Having less parallelization can increase the total migration completion time.
The default value, `10`, is fine for most of the setups.

|`hazelcast.partition.max.parallel.replications`
|10
|int
|Maximum number of parallel partition backup replication operations per member.
When a partition backup ownership changes or a backup inconsistency is detected, the members start to sync their backup partitions.
This parameter limits the maximum running replication operations in parallel.
The default value, which is the value of `hazelcast.partition.max.parallel.migrations`, is fine for most of the setups.

|`hazelcast.partition.migration.chunks.enabled`
| true
| bool
| Subdivides the migration fragments into chunks to prevent
  out-of-memory errors during large partition migrations.

|`hazelcast.partition.migration.chunks.max.migrating.data.in.mb`
| 250
| int
| Limits the total size of migration data in MB for a single
  partition migration operation. If you have parallel migrations,
  the total size of migration data in MB is limited to the
  number of parallel migrations multiplied by this value.

|`hazelcast.partition.migration.fragments.enabled`
| true
| bool
| When enabled, which is the default behavior, partitions are migrated/replicated in small fragments instead of one big chunk.
Migrating partitions in fragments reduces pressure on the memory and network
since smaller packets are created in the memory and sent through the network.
Note that it can increase the migration time to complete.

|`hazelcast.partition.migration.interval`
| 0
| int
| Interval to run partition migration tasks in seconds.

|`hazelcast.partition.migration.stale.read.disabled`
| false
| bool
| Hazelcast allows read operations to be performed while a partition is being migrated.
This can lead to stale reads for some scenarios.
You can disable stale read operations by setting this system property's value to "true".
Its default value is "false", meaning that stale reads are allowed.

|`hazelcast.partition.migration.timeout`
| 300
| int
| Timeout for partition migration tasks in seconds.

|`hazelcast.partition.table.send.interval`
|15
|int
|Interval for publishing partition table periodically to all cluster members in seconds.

|`hazelcast.partitioning.strategy.class`
|
|string
|Class name implementing `com.hazelcast.core.PartitioningStrategy`, which defines key to partition mapping.

|`hazelcast.persistence.auto.cluster.state`
| true
| bool
| Enables/disables xref:kubernetes:kubernetes-persistence.adoc[automatic state management] of clusters in Kubernetes Statefulset, which are configured with persistence enabled.
When enabled, Hazelcast monitors the runtime environment to detect the intent of shutdown and automates the cluster state management decisions.

|`hazelcast.persistence.auto.cluster.state.strategy`
| string
| `NO_MIGRATION`
| Selects the xref:kubernetes:kubernetes-auto-discovery.adoc#configuration[cluster state] to be used when members are temporarily missing from a cluster in Kubernetes Statefulset which is configured with persistence enabled. Valid values are `NO_MIGRATION` and `FROZEN`.

|`hazelcast.phone.home.enabled`
| true
| bool
| Enable or disable the sending of phone home data to Hazelcast's phone home server.

|`hazelcast.prefer.ipv4.stack`
| true
| bool
| Prefer IPv4 network interface when picking a local address.

|`hazelcast.query.max.local.partition.limit.for.precheck`
|3
|int
|Maximum value of local partitions to trigger local pre-check for `Predicates#alwaysTrue()`
query operations on maps.

|`hazelcast.query.optimizer.type`
|RULES
|String
|Type of the query optimizer. For optimizations based on static rules, set the value to `RULES`.
To disable the optimization, set the value to `NONE`.

|[[parallel-predicates]] `hazelcast.query.predicate.parallel.evaluation`
|false
|bool
|Each Hazelcast member evaluates query predicates using a single thread by default.
In most cases, the overhead of inter-thread communications overweight can benefit from parallel execution.
When you have a large dataset and/or slow predicate, you may benefit from parallel predicate evaluations.
Set to `true` if you are using slow predicates or have > 100,000s entries per member.

|`hazelcast.query.result.size.limit`
|-1
|int
|Result size limit for query operations on maps.
This value defines the maximum number of returned elements for a single query result.
If a query exceeds this number of elements, a `QueryResultSizeExceededException` is thrown.
Its default value is -1, meaning it is disabled. See xref:data-structures:preventing-out-of-memory.adoc#setting-query-result-size-limit for a detailed explanation
of setting a result size limit.

|`hazelcast.security.recommendations`
|null
|string
|When set to any non-null value, security recommendations are logged at the `INFO` level during the member startup.

|`hazelcast.serialization.version`
|
|long
|Version of the Hazelcast serialization. Accepted values are between 1 and
the highest supported serialization version.

|[[hazelcast.shutdownhook.enabled]]`hazelcast.shutdownhook.enabled`
| true
| bool
| Enables/disables Hazelcast shutdownhook thread.
When enabled, the shutdown action is determined according to the value of <<hazelcast.shutdownhook.policy , `hazelcast.shutdownhook.policy`>>.


|[[hazelcast.shutdownhook.policy]]`hazelcast.shutdownhook.policy`
|TERMINATE
|string
| Specifies the behavior when JVM is exiting while the Hazelcast instance is still running.
It has two values: TERMINATE and GRACEFUL. The former one terminates the Hazelcast instance immediately.
The latter, GRACEFUL, initiates the graceful shutdown which can significantly slow down the JVM exit process, but it tries to retain data safety.

NOTE: Always shutdown Hazelcast explicitly using the `HazelcastInstance.shutdown()` method; see xref:maintain-cluster:shutdown.adoc#graceful-shutdown[Graceful Shutdown]. It's not recommended to rely on the shutdown hook, this is a last-effort measure.

|`hazelcast.slow.operation.detector.enabled`
|true
|bool
|Enables/disables the xref:performance:slowoperationdetector.adoc[SlowOperationDetector].

|`hazelcast.slow.operation.detector.log.purge.interval.seconds`
|300
|int
|Purge interval for slow operation logs.

|`hazelcast.slow.operation.detector.log.retention.seconds`
|3600
|int
|Defines the retention time of invocations in slow operation logs.
If an invocation is older than this value, it is purged from the log to prevent unlimited memory usage.
When all invocations are purged from a log, the log itself is deleted.

|`hazelcast.slow.operation.detector.stacktrace.logging.enabled`
|false
|bool
|Defines if the stacktraces of slow operations are logged in the log file.
Stack traces are always reported to the Management Center, but by default, they are not printed to keep the log size small.

|`hazelcast.slow.operation.detector.threshold.millis`
|10000
|int
|Defines a threshold above which a running operation in `OperationService` is considered to be slow.
These operations log a warning and are shown in the Management Center with detailed information, e.g., stacktrace.

|`hazelcast.socket.bind.any`
| true
| bool
| Bind both server-socket and client-sockets to any local interface. For ZIP and TAR distributions, this is overridden by false in configuration files.

|`hazelcast.socket.buffer.direct`
| false
| bool
| Specifies whether the byte buffers used in the socket should be a direct byte buffer (`true`) or a regular one (`false`).
When it is set to `true`, Hazelcast internally uses the method `ByteBuffer.allocateDirect` (instead of `ByteBuffer.allocate`) which makes use of
the off-heap and may skip the memory copying when performing socket I/O operations.
See link:https://docs.oracle.com/javase/7/docs/api/java/nio/ByteBuffer.html[here^] for more information.

|`hazelcast.socket.client.bind`
|true
|bool
|Bind client socket to an interface when connecting to a remote server socket.
When set to `false`, client socket is not bound to any interface.

|`hazelcast.socket.client.bind.any`
| true
| bool
| Bind client-sockets to any local interface. If not set, `hazelcast.socket.bind.any` is used as the default.

|`hazelcast.socket.client.buffer.direct`
|false
|bool
|Specifies whether the byte buffers used in the socket, when connecting to a client, should be direct or a regular one.

|`hazelcast.socket.client.receive.buffer.size`
|-1
|int
|Hazelcast creates all connections with receive buffer size set according to the `hazelcast.socket.receive.buffer.size`.
When it detects a connection opened by a client, then it adjusts the receive buffer size according to this property.
It is in kilobytes and its default value is -1.

|`hazelcast.socket.client.send.buffer.size`
|-1
|int
|Hazelcast creates all connections with send buffer size set according to the `hazelcast.socket.send.buffer.size`.
When it detects a connection opened by a client, then it adjusts the send buffer size according to this property.
It is in kilobytes and its  default value is -1.

|`hazelcast.socket.connect.timeout.seconds`
|10
|int
|Socket connection timeout in seconds. `Socket.connect()` is blocked until
either connection is established or connection is refused or this timeout passes.
Default is 10 seconds, 0 means infinite.

|`hazelcast.socket.keep.alive`
| true
| bool
| Sets a socket to be in the `SO_KEEPALIVE` option.

|`hazelcast.socket.linger.seconds`
|0
|int
|Sets the linger value (timeout) for a socket in the `SO_LINGER` option.

|`hazelcast.socket.no.delay`
| true
| bool
| Specifies whether the delay of sending successive small packets on the network is disabled.

|`hazelcast.socket.receive.buffer.size`
| 128
| int
| Socket receive buffer (`SO_RCVBUF`) size in KB.
If you have a very fast network, e.g., 10gbit) and/or you have large entries, then you may benefit from increasing sender/receiver buffer sizes.
Use this property and the next one below tune the size.

|`hazelcast.socket.send.buffer.size`
| 128
| int
| Socket send buffer (`SO_SNDBUF`) size in KB.

|`hazelcast.socket.server.bind.any`
| true
| bool
| Bind server-socket to any local interface. If not set, `hazelcast.socket.bind.any` is used as the default.

|[[tcp-try-count]]`hazelcast.tcp.join.port.try.count`
|3
|int
|The number of incremental ports, starting with the port number defined in the network configuration,
that is used to connect to a host (which is defined without a port in TCP/IP member list while a member is searching for a cluster).

|`hazelcast.tcp.join.previously.joined.member.address.retention.seconds`
|14400
|int
|Specifies how long the address of a member that has previously joined the cluster will be retained/remembered in the TCP/IP joiner, after it leaves the cluster. In a split-brain scenario, this member addresses
is used to discover the other cluster by the split-brain handler.

|`hazelcast.wait.seconds.before.join`
| 5
| int
| Wait time before join operation.
This time establishes a pre-join phase time window for newcomer members to
make their first join requests. Once `hazelcast.wait.seconds.before.join`
elapses since the last first-timer join request (i.e., where the member hasn't
made any previous join request), or the pre-join phase has lasted for
`hazelcast.max.wait.seconds.before.join` seconds, the phase ends and the
master starts forming the cluster. See `hazelcast.async.join.strategy.enabled` for how this behaviour can change.

|`hazelcast.wan.consumer.ack.delay.backoff.init`
|1
|int
|Defines the initial backoff delay for the WAN implementation's consumer. It is used if the <<ack-delay, acknowledgment delaying>> feature is enabled.

|`hazelcast.wan.consumer.ack.delay.backoff.max`
|100
|int
|Defines the maximum backoff delay for the WAN implementation's consumer. It is used if the <<ack-delay, acknowledgment delaying>> feature is enabled.

|`hazelcast.wan.consumer.ack.delay.backoff.multiplier`
|1.5D
|string
|Defines the multiplier (the speed of the function) for the backoff delay for the WAN implementation's consumer. It is used if the <<ack-delay, acknowledgment delaying>> feature is enabled

|`hazelcast.wan.consumer.invocation.threshold`[[ack-delay]]
|50000
|int
|Defines the pending invocation threshold for the WAN replication implementation. Exceeding this threshold on a WAN consumer member makes the member to delay the WAN acknowledgment,
thus slowing down the WAN publishers on the source side that send WAN events to the given WAN consumer. Setting this value to negative disables the acknowledgment delaying feature.

|`hazelcast.wan.replicate.imap.evictions`
|false
|bool
|Specifies whether `IMap` objects replicate entry evictions over WAN to target clusters. Use this property with caution. When enabled, _ALL_ evictions, including size or expiration-driven evictions and manual invocations are replicated to target clusters. Setting this property to `true` does not guarantee data consistency between clusters.

|`hazelcast.wan.replicate.icache.evictions`
|false
|bool
|Specifies whether `ICache` objects replicate entry evictions over WAN to target clusters. Use this property with caution. When enabled, _ALL_ evictions, including size or expiration-driven evictions and manual invocations are replicated to target clusters. Setting this property to `true` does not guarantee data consistency between clusters.

|`tcp.channels.per.connection`
| 1
| int
| Number of parallel connections between members. Not having more connections than IO threads is recommended.

|`hazelcast.async.join.strategy.enabled`
| true
| bool
| Controls the behaviour of `hazelcast.wait.seconds.before.join` and `hazelcast.max.wait.seconds.before.join`. If set to `true`, joining members will return immediately without blocking. The cluster remains in the same state until the configured timeouts have elapsed, and new members will behave like lite members. After the timeouts have elapsed and no new members are queued to join, cluster repartitioning is performed in the background. This async strategy reduces latency significantly for new members joining clusters.


|===