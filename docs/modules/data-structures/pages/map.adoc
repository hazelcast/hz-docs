= Distributed Map
:description: Distributed maps, also known as maps or IMaps, are key-value pairs that are partitioned across a cluster. Use maps to distribute data across a cluster and make it easy to scale your applications while protecting data against member failures.
:url-cap-theorem: https://en.wikipedia.org/wiki/CAP_theorem
[[map]]

{description}

Distributed maps store values that you can access, using a unique key. These values can be primitives, serialized objects, or JSON values. In the following example, the map stores the names of captial cities as simple strings. The key is an incrementing integer to make sure that each entry is unique.

```
1. Tokyo
2. Paris
3. London
4. Ankara
5. Berlin
```

== How Map Entries are Distributed

Hazelcast uses the keys to distribute map entries across the cluster. Each member in a cluster stores an almost equal number of entries.
For example, if you have a map with 1000 entries on one member, then you start a second member, each member will store 500 entries. To learn more about how data is partitioned in a cluster, see xref:architecture:data-partitioning.adoc[].

== What Makes Maps Fault Tolerant

By default, all maps are backed up by one other member to avoid data loss. If a member goes down, the members holding the backup data take over. Backup copies are then redistributed across remaining cluster members.

== Availability vs Consistency

In terms of the link:{url-cap-theorem}[CAP theorem], distributed maps are designed to ensure that data is _available_ rather than _consistent_. However, you can configure maps to xref:managing-map-memory.adoc[reduce the chances of data becoming stale].

== Next Steps

Find out how to xref:map-config.adoc[configure maps]. You can then xref:creating-a-map.adoc[create a map] that inherits your configuration.
